# L01 - 机器学习范式｜探索性机器学习｜实验设计



### 第一部分：Paradigms of Machine Learning 机器学习范式（相关概念）

机器学习范式是指机器学习方法的基本类型和风格，它们各自代表了不同的方法和算法来从数据中学习。

#### Supervised learning 监督学习

什么是监督学习：从X数据集，估算/预测得到结果Y

监督学习的分类：

- Classification Problem 分类问题:
  - Y是离散的Class（类别），例如：判断一个实例是猫还是狗。
- Regression Problem 回归问题（又称：建模/函数逼近）:
  - Y是连续的（线性），例如：预测时间序列的下一个值。

#### Unsupervised learning 无监督学习

什么是无监督学习：不考虑/根本没有Y（无需输出的问题）

无监督学习的分类：

​	目的是：将X数据集分组，或者挖掘数据中的关联规则；

- Clustering 聚类:
  - 聚类问题是无监督学习的一个重要方面，目标是识别实例的自然聚合，例如：通过相似性将数据点分组。
- Association Rule Mining 关联规则挖掘:
  - 关联规则挖掘旨在发现数据中的强模式，例如：购物篮分析中的频繁项目集。

#### Reinforcement learning 强化学习
- 强化学习涉及训练一个代理（agent）感知环境并作出响应，在每次响应动作后，会收到奖励或惩罚，代理通过结果学习如何响应能最大化某种奖励，通过试错学习最佳行为策略。



### 第二部分：Exploratory Data Analysis 探索性数据分析
#### Motivation 动机

为什么需要 Exploratory Data Analysis？

- 探索性数据分析的动机在于处理数据中的质量问题，如冗余、不相关、噪声和一致性问题，以及不完整、错误或不一致的数据，从而更好的实现数据挖掘的目的。

#### Data properties exploration 数据属性探索

这包括数据的统计描述，如中心趋势、分散度、分布形状等，以及对数据特征之间关系的探索。

##### Complexity metrics for classification methods （数据集）复杂度衡量标准

：如何通过不同的度量标准来量化分类问题的困难度，从而帮助研究者和从业者理解和改进分类算法。这部分主要介绍了Basu和Ho2002年提出的复杂性度量标准的研究工作。

1. 量化的复杂性度量，举例说明：每个都代表了一种可以量化的复杂性度量
   - 线性可分性程度（Degree of linear separability）：表示可以通过线性分类器分离不同类别的程度。
   - 通过生成树计算的类边界长度（Length of the class boundary by means of spanning trees）：指的是类边界的复杂性，可以通过生成树来计算。
   - 类流形的形状（Shape of class manifolds）：关于类在多维空间中的分布形状和流形的复杂性。
2. 度量标准的组:
   - 个别特征的重叠度量（Measures of overlap of individual features）：量化数据集单个属性的区分能力。
   - 类的可分性度量（Measures of separability of classes）：多变量度量，考虑所有属性来共同评估类的可分性。
   - 流形的几何、拓扑和密度度量（Measures of geometry, topology, and density of manifolds）：这些度量分析使用不同技术的类边界形状。
   - ...一共有12个组别；

#### Quality control tools 质量控制工具/技术

介绍：对数据集执行质量控制的自动工具，比如：**Pandas Profiling**（Python库，一个简单的脚本）

- 用于自动化质量控制
- 检查数据问题，如大量零值、唯一值、缺失值、数据分布的偏斜和特征间的相关性
- 生成广泛的解释性统计数据
- 自动生成优秀的HTML report

```python
import numpy as np
import pandas as pd
from pandas_profiling import ProfileReport

df = pd.read_csv("dataset.csv")
profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}},minimal=True)
profile.to_file(output_file="your_report.html")
```

​	这段代码使用了Python的Pandas库和Pandas Profiling包来生成数据集的统计概述报告。它首先导入所需的库，然后读取一个CSV文件到Pandas DataFrame对象。接着，使用`ProfileReport`功能创建一个配置为“最小化”的数据概述报告，这意味着报告会生成更少的信息，可能仅包含主要的统计数据和图表。最后，报告以HTML格式保存到指定的文件中。这个过程可以帮助数据科学家快速理解数据集的主要特征，包括数据类型、缺失值、数据分布等。



### 第三部分：Experimental Evalution 实验评估

#### Motivation 动机

为什么需要Experimental Evalution？

- 如何评估这些机器学习算法在给定数据集上的执行情况以及哪种算法最合适？有非常精确定义的协议。
- 实验设计的目的是验证模型的有效性和性能，确保结果和结论的可靠性。

#### Performance Metrics 评估对应的性能指标

##### 监督学习（分类 回归）

（1）Classification 分类问题的性能指标：

- 准确率：定义为C/D，其中C是正确分类的示例的数量，D是实例集的大小。它是最简单且使用最广泛的指标。如果你有100个样本，其中95个被正确分类，准确率就是95%。

- Cohen 的 Kappa：它衡量两个分类变量分布（如真实类别和预测类别）之间的一致性，并解释偶然的一致性。它更适合多类问题。当你有一个平衡的数据集（即每个类别的样本数量大致相等）或者多类分类问题时，这个指标特别有用。Kappa值1表示完美一致，0表示没有比随机分配更好的一致性，而负值表示比随机分配还差。

​	附加指标：

- 精度：这个指标衡量的是模型预测为正类的样本中实际为正类的比例。例如，如果一个模型预测出10个正类样本，但其中只有7个是真正的正类，那么精确率就是70%。

- 召回率：也称为敏感性，它衡量的是所有实际正类中被模型正确预测的比例。如果有10个真正的正类样本，模型只正确预测了7个，那么召回率是70%。

- F1 分数：精度和召回率的调和平均值，计算方法为精度和召回率乘积的 2 倍除以精度和召回率之和。试图在两者之间找到一个平衡。F1分数是精确率和召回率的乘积的两倍除以它们的和。如果模型的精确率和召回率都高，F1分数也会很高。
- AUPRC：精确率-召回率曲线下的面积。这个指标对于类别不平衡的数据集特别有用。它显示了在不同的阈值设置下，模型的性能如何变化。

（2）Regression 回归问题的性能指标：

- RMSD：均方根偏差，衡量模型预测值与实际值之间误差的平均大小。它是预测与实际观察之间的平方差平均值的平方根。http://en.wikipedia.org/wiki/RMSD
- R2，决定系数：这是回归预测与真实数据点的近似程度的统计度量。从自变量预测的因变量方差的比例。它提供了模型复制观察结果的良好程度的衡量标准。https://en.wikipedia.org/wiki/Coefficient_of_determination

##### 无监督学习（聚类 关联规则挖掘）

（3）聚类问题的指标：http://en.wikipedia.org/wiki/Cluster_analysis

（4）关联规则挖掘的指标：

​	这两个指标通常用于“频繁项集挖掘”和“关联规则生成”阶段，这是关联规则挖掘中的两个主要步骤；

- 支持度：这是该模式覆盖的实例的百分比。

  - 在实际应用中，支持度可以用来筛选那些至少满足最小支持度阈值的项集。只有当一个项集的支持度不低于用户定义的最小支持度阈值时，这个项集才会被认为是“频繁的”，并被用于进一步分析或构建关联规则。

- 置信度：XUY/X，表示当X发生时，XUY发生的可能性很大。通常，只有当一条规则的置信度不低于用户定义的最小置信度阈值时，这条规则才会被认为是有趣的或者值得关注的。

  

  

#### Performance estimation methodologies 性能估算方法

##### 1. 数据集的划分方法

###### Holdout

- 数据集分为两个不重叠的集合，通常使用 2/3 等比例进行训练，1/3 进行测试。
- 这是一种简单且计算成本低廉的方法。
- 模型性能在测试集上进行评估。
- 该方法的可靠性很大程度上取决于集合的划分方式。

###### K-fold交叉验证

​	此方法比保留方法更系统，因为它确保数据集中的每个示例都有机会在测试集中出现一次。由于模型在不同的数据子集上进行了 K 次训练和测试，因此所得的性能估计通常更可靠并且更少依赖于数据分区。然而，它的计算成本更高，特别是对于大型数据集和/或复杂模型。

###### Leave-one-out交叉验证

​	LOOCV 是交叉验证的一种极端形式，计算量非常大，因为模型必须训练 D 次（其中 D 是数据点的数量）。通常不建议用于大型数据集，但当数据集非常小时，它可能很有用，因为它可以充分利用可用数据。

###### Bootstrap方法

- 从数据集中替换生成样本，并将该样本用作训练集。
- 从未选择的实例用作测试集。
- 这个过程被重复很多次。

###### 如何选择方法？

在选择方法时，主要取决于：偏差（Bias）和方差（Variance），以及数据集的特性。

###### 	· 偏差（越低越好）

​	预测值与真实值之间的差异。高偏差意味着模型的预测值与真实值相差很大，表明模型过于简单，没有很好地捕获数据的复杂性，这种现象称为**欠拟合**。

###### 	· 方差（越低越好）

​	描述的是模型对给定数据的预测值变化的广度。高方差意味着模型对训练数据的微小变化非常敏感，可能导致模型在新数据上表现不稳定，这种现象称为**过拟合**。

##### 2. 机器学习背景下的模型选择和参数设置

###### 选型及参数设置

- 构建机器学习模型或管道时，需要做出几个决定，例如：
  - 使用哪个分类器（例如，分类器 A 或 B）。
  - 应该为分类器设置哪些参数（例如，决策树的深度）。
- 原则是做出能够产生更好预测的模型的决策。
- 然而，测试集不应该用于做出这些决定，因为这就像作弊。测试数据仅应用于评估训练模型的预测性能。

###### 模型选择：数据分区

- 训练/验证/测试：数据集被分成不重叠的集合，用于训练、验证（调整模型）和测试（评估模型性能）。
- 嵌套交叉验证
  - 涉及交叉验证过程创建的数据集的初始分割，然后通过交叉验证将其进一步划分为多个训练和测试集。
  - 内部交叉验证用于做出有关模型的决策。
  - 最终模型是使用整个训练集构建的，并在测试集上进行评估。
  - 这个过程的计算成本很高，因为它涉及训练K^2个模型（其中 K 是交叉验证中的fold个数）。

###### 模型选择：如何做出决策

- 网格搜索
  - 如果有 N 个参数要测试，并且每个参数可以取 M 个不同的值，则网格搜索涉及生成所有可能组合的实验，
- 智能探索（AutoML）
  - **Auto-sklearn**：利用贝叶斯优化，它定义了一个概率模型来根据参数值估计性能，并根据使用这些参数的机器学习算法的性能迭代地细化选择。https://automl.github.io/auto-sklearn/master/
  - **TPOT**：依赖于遗传编程，其中每棵树都指定一个完整的机器学习管道，并可选择算法和参数。（IoT使用的ML模型）https://epistasislab.github.io/tpot/

现在最火的：https://scikit-learn.org/stable/ scikit-learn，the most popular ML library nowadays；



### 第四部分：看一个实例

```python
import os
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

# Load iris dataset（倒入数据集：X测试集 y验证集）
X, y = load_iris(return_X_y=True)

# Create a Decision Tree Classifier（使用默认参数实例化）
classifier = DecisionTreeClassifier()

# Define the parameter grid to search（为决策树定义参数网格）
param_grid = dict(max_depth=[2, 3, 4, 5, 10])

# List to store scores
scores = []

# Create KFold cross-validation（设定5倍交叉验证，交叉验证方法之一，参数优化技术）
kf = KFold(n_splits=5, shuffle=True)

# Perform cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # Perform GridSearchCV（用于对决策树的指定参数值执行穷举搜索）
    grid_search = GridSearchCV(classifier, param_grid=param_grid, cv=5, scoring="f1_weighted")
    grid_search.fit(X_train, y_train)
    
    # Print the score of the best estimator
    print("Score of best estimator {0}".format(grid_search.best_score_))
    
    # Get the best estimator
    estimator = grid_search.best_estimator_
    print("Max depth {0}".format(estimator.max_depth))
    
    # Predict and calculate the score
    pred = estimator.predict(X_test)
    score = metrics.f1_score(y_test, pred, average="weighted")
    scores.append(score)
    print("Score of best model on test fold {0}".format(score))

# Print the average cross-validation score
print("Average cross-validation score {0}".format(np.average(scores)))

```

​	此代码使用决策树分类器在 Iris 数据集上执行 k 倍交叉验证并结合网格搜索进行超参数调整。它使用`f1_weighted`评分指标来评估模型的性能。交叉验证后，它打印每次折叠的最佳估计器的分数和平均交叉验证分数。

​	网格搜索（Grid Search）结合交叉验证（Cross-Validation）是一种常用于机器学习模型的参数优化技术。其目的主要包括：

1. **参数优化**：  

   ​	网格搜索会遍历所有指定的参数组合，为每种可能的参数组合训练模型，从而找到最优的参数设置。这是一个自动化的过程，可以帮助数据科学家确定最佳的参数组合，而不需要手动更改和测试每个参数。 

2. **模型验证**：

   ​	交叉验证通过将数据集分割成多个小组（称为“折”），确保模型在不同子集的数据上都能得到验证。这有助于评估模型的泛化能力，即模型对未见过的数据的预测能力。 

3. **防止过拟合**：  

   ​	交叉验证通过在不同的数据折上重复训练和验证，减少了模型因偶然性在一次数据划分中表现良好的概率，从而降低过拟合的风险。 由于最终选择的参数不是基于单一数据集的表现，而是基于在多个数据子集上的平均表现，所以选择的模型更加稳健。 

4. **结果稳定性**： 

   ​	交叉验证产生的多个评估指标的平均值提供了一个比单一数据集划分更可靠的性能估计。这样的平均结果通常更稳定，不太受到数据抽样偏差的影响。 

5. **时间和资源的有效利用**：

   ​	网格搜索结合交叉验证虽然计算成本高，但它是一种无需持续监督的自动化过程，可以在计算资源允许的情况下并行执行。 它最大限度地利用了有限的数据，特别是在数据量不大的情况下。 

总的来说，网格搜索结合交叉验证是一种强大的工具，用于在机器学习中找到最优的模型参数，同时确保模型的泛化性和稳定性。



# Practical

实验环境：Google Co-lab

实验材料：Jupyter 笔记本 (CSC8111-Practical1.ipynb) 和数据集 (checkerboard.csv)；



这项实践练习旨在使用真实世界的数据集和流行的 Python 库，提供机器学习中探索性数据分析和实验评估的实践经验。

1. **内容**：它扩展了第 2 讲中的交叉验证/网格搜索脚本，进行了重大更改，例如从 CSV 文件加载数据、测试两个参数的组合、使用 TPOT（AutoML 系统）以及生成精确召回曲线。
2. **数据集详细信息**：checkerboard.csv 文件包含 993 个实例和 20 个属性。它旨在仅使用两个属性（att3 和 att4）创建棋盘图案，而其他 18 个属性是由高斯函数生成的噪声。
3. **代码探索**：笔记本包含注释代码行，允许用户在分类器（随机森林、K 最近邻、TPOT AutoML）之间切换，并选择是使用全套属性还是仅使用两个相关属性。
4. **精确回忆曲线绘制**：添加了一个新单元（单元 9），用于生成精确回忆曲线来表示分类器在数据集上的性能。
5. **建议的任务**：鼓励用户尝试不同的分类器、调整参数、尝试仅使用相关属性，并探索 Scikit-Learn 网站上提供的其他性能指标。



对于这个实验重点关注：

1. **数据理解与预处理**：首先要熟悉checkerboard.csv数据集的特性，理解其中的20个属性中哪些是关键属性，哪些是噪声。这涉及到数据探索和预处理的基本技能。
2. **模型选择与调参**：实验要求你探索不同的分类器（如随机森林、K-近邻、TPOT AutoML），并对它们进行参数调优。这是机器学习中非常重要的环节，关系到模型的性能和效果。
3. **特征选择**：你需要尝试使用所有属性以及仅使用关键属性两种情况。这可以帮助你理解特征选择对模型性能的影响。
4. **性能评估**：实验中加入了生成精确率-召回率曲线的任务。你应该学会如何生成这种曲线，并理解它如何评估分类器的性能。
5. **探索与实验**：实验鼓励你进行探索和实验。你可以尝试不同的参数组合，使用不同的性能度量标准，甚至可以探索Scikit-Learn提供的其他模型和技术。
6. **理论与实践结合**：此实验不仅是对机器学习理论知识的应用，也是对实际编程和数据分析技能的锻炼。在完成实验的同时，理解背后的理论知识是很重要的。

总结来说，这个实验的关键在于理解和应用机器学习的基本流程，包括数据预处理、模型选择与调参、特征选择和性能评估，以及将理论知识应用于实际问题中。通过这个实验，你不仅可以提升技术能力，还能更深入地理解机器学习的核心概念。

