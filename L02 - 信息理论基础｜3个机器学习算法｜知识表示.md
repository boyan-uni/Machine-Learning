# L02 - 信息理论基础｜3个机器学习算法｜知识表示



本文档详细概述了三种机器学习算法：决策树、随机森林和最近邻分类器。

- **决策树**：类似流程图的结构，其中每个内部节点代表对属性的测试，每个分支代表测试的结果，每个叶节点代表一个类标签。从根到叶子的路径代表分类规则。
- **随机森林**：一种集成学习方法，通过在训练时构建大量决策树并输出作为各个树的类的模式的类来进行操作。它通过使用特征和样本的子集引入随机性。
- **最近邻分类器**：一种基于实例的学习，实际上并不创建通用内部模型，而是使用整个数据集进行预测。例如，k 最近邻算法根据数据集中找到的 k 个最相似实例的多数票来预测样本的分类。

这些方法是机器学习的基础，并为分类和回归任务提供了不同的方法。决策树和随机森林对于可解释性特别有用，而最近邻方法对于数据点自然聚类成相似项目组的数据集非常有用。



## 信息理论

​	帮助确定在构建决策树等模型时应如何选择特征来最佳地预测数据。通过计算信息增益，可以评估每个特征对于数据集分类的贡献度，选择那些提供最多信息的特征进行分割，以此构建模型。

1. Shannon’s Entropy（香农熵）

   - 它是用来衡量离散数据中信息含量的一个度量。

   - 它的灵感来自于统计信息传输。

   - 一个实例是，如果我们有一个硬币投掷序列要发送，当正反面概率相等时，每次投掷需要1比特来传输；但如果某一面出现的概率非常小，我们只需发送少数次出现的那一面的信息，其他默认为常见的一面，平均来看传输的信息会少于1比特。

2. Conditional Entropy（条件熵）
   - 在已知与某变量相关的另一变量的信息的条件下，该变量的熵。

3. Information Gain（信息增益）

   - 如果我们知道X，那么在传输Y时可以节省多少比特。

   - 信息增益是通过公式 IG(Y|X) = H(Y) – H(Y|X) 来计算的，其中H(Y)是Y的熵，H(Y|X)是已知X的条件下Y的条件熵。



## 三大机器学习算法：决策树、随机森林、最近邻分类器（级级递进关系）

### Decision Tree 决策树

##### 1. 图示

<img src="https://cdn.jsdelivr.net/gh/boyan-uni/pic-bed/img/MachineLearning-L02-P1.png" alt="image-20240812155735899" style="width:50%;" />

##### 2. 算法：Decision Tree Induction 决策树归纳算法

​	是一种贪心算法，通过自顶向下的递归分而治之方法来构建树。所有训练样本开始时都位于根节点，属性被分类（如果是连续值的话则提前离散化），然后根据选定的属性递归地划分样本。测试属性的选择基于启发式或统计度量，例如信息增益。

##### 3. 如何在构建决策树时通过信息增益（Information Gain）来选择 “最好的属性” 决定数据如何分割？

​	信息增益是一个基于熵的概念，用于量化选择某个属性来划分数据集时带来的“信息”增加量。在ID3或C4.5决策树算法中，选择具有最高信息增益的属性作为节点的划分属性。

###### · IG计算方法（Information Gain 信息增益）

​	这是在ID3和C4.5决策树算法中使用的一个度量。它的计算方法如下：

​	<img src="https://cdn.jsdelivr.net/gh/boyan-uni/pic-bed/img/MachineLearning-L02-P2.png" alt="image-20240812155848311" style="width:80%;" />

​	通俗易懂的解释：

​	想象我们有一群小动物，有兔子和刺猬，我们想根据它们的特征来分组。我们有两个属性可以选择：一个是“大小”（大或小），另一个是“速度”（快或慢）。

​	首先，我们计算整个动物群体的“熵”——也就是它的混乱度。如果所有的动物都是兔子或都是刺猬，那么这个群体就一点也不混乱（熵为0），因为我们不需要任何信息就能确定任一动物是什么。但如果群体里兔子和刺猬各占一半，我们就需要更多的信息来确定一个随机动物的种类（熵更高）。

​	然后，我们看看每个属性如何分割这群动物。我们问：“如果我只看大小，我能更好地分辨出兔子和刺猬吗？”我们按大小分成两组，计算每组的熵，然后看看整体的熵是否减少了。我们对“速度”属性做同样的事情。

​	最后，信息增益就是原来的群体熵减去我们按某个属性分组后的熵。哪个属性有最大的信息增益，就意味着按这个属性分组后，我们减少了最多的不确定性，或者说，我们获得了最多的“信息”，这就是我们决定按哪个属性来分割数据的依据。

###### · Gini 不纯度（Gini Impurity）

替代的分割标准：https://victorzhou.com/blog/gini-impurity/

##### 4. 问题：过拟合（Overfitting） and  解决方法：决策树剪枝（Tree Pruning）

**过拟合和树的剪枝**：

- 过拟合描述了一个模型复杂到可以完美描述训练数据的每一个细节，包括噪声和异常值，但它可能无法很好地推广到未见过的数据。
- 为了避免过拟合，可以采用两种方法：
  - **预剪枝**：在树的构建过程中提前停止树的生长。如果继续分割节点不会增加模型的性能，就停止分割。
  - **后剪枝**：在树完全生长后，移除那些提高不了模型在独立数据集上性能的分支。

##### 5. 应用：一段Python代码来演示如何实践这些概念

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

# Load the Iris dataset 加载数据集
X, y = load_iris(return_X_y=True)

# Initialize the classifier with some parameters 初始化分类器，使用初始化参数
classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=5, min_samples_leaf=3)

# Perform cross-validation 交叉验证
scores = []
kf = KFold(n_splits=5, shuffle=True)	# 把数据分成5部分，每次测试留下一组空白操作，用于后面给出评价，shuffle设置在分堆前随机洗牌以保证公平

for train_index, test_index in kf.split(X):
  	# kf.split(X)根据 数据集X 给出了不同的组合方式，然后通过for循环每次获取kf.split(X)中的每一条数据项，分成对应的4种用途的数据：X_train, X_test, y_train, y_test
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # Fit the model on the training data, Classifier.使用训练数据 ‘X/y_strain’ 来训练或建立模型
    classifier.fit(X_train, y_train)
    
    # Predict on the test data, Classifier.使用 ‘X_test’ 来检查pred（预测测试数据）。
    pred = classifier.predict(X_test)
    
    # Calculate the accuracy score, 评分：通过比较pred和正确答案y_test，计算出一个F1分数（累计），反应此pred的准确度。
    score = metrics.f1_score(y_test, pred, average="weighted")
    scores.append(score)

# Print the average cross-validation score 打印
print("Average cross-validation score {0:.2f}".format(np.average(scores)))
```

​	这段代码加载了著名的鸢尾花（Iris）数据集，创建了一个决策树分类器，并用交叉验证来评估模型的性能。这个例子使用了`f1_score`作为性能评估的指标，计算了加权平均的F1分数，最后输出了交叉验证得到的平均分数。

##### 6. 引入RF算法：集成学习及其在生成模型多样性方面的重要性

###### · 什么是集成学习 (Ensemble Learning)？

**集成学习** 就像是请一群专家而不只是一个人来解决同一个问题。每个专家都会给出他们的意见（预测），然后我们通常采用大多数人的意见作为最终决定（多数投票）。这样做的好处是可以减少犯错的机会，因为即使有些专家犯了错误，其他专家的正确意见也会弥补这个缺陷。

但是，如果所有的专家都受到相同的培训，他们可能会犯同样的错误。这就是为什么我们需要**多样性**。如果每个专家都有独特的专长和经验，那么他们就不太可能集体犯同样的错误。

在机器学习中，我们通过以下方法来培养这种多样性：

1. **Bagging（装袋）**：就像给每个专家提供一个稍微不同的题库来进行准备。因为每个专家都看到了不同的问题，所以他们会形成不同的意见。这就像是随机从一堆问答卡中抽出一部分给每个专家，有的卡片可能会被重复抽到，有的则可能一次也没抽到。
2. **Boosting（提升）**：这就像是一个有序的培训课程，每一个后续的专家都会重点关注前一个专家没有回答好的问题。这样，每一个新的专家都会试图补上这个缺口，整体上，群体的表现会因此提高。
3. **通过超参数调整生成多样性**：这就像是调整每个专家的培训课程的特定方面，比如有些专家在统计学上受训更多，有些则可能在解决问题的速度上更快。通过微调他们的培训计划（模型的超参数），我们可以确保他们在各自的专长领域中表现最好。

通过这样做，我们可以确保即使某些模型在某些方面出错，其他模型的正确预测也会帮助我们获得整体上更可靠、更准确的结果。这就是集成学习能够通常比单一模型表现更好的原因。

###### · 什么是超参数？

​	在机器学习中，超参数是在学习过程开始之前设置的参数，不同于模型训练过程中学习的参数。简单来说，如果把机器学习模型比作一辆车，那么超参数就像是你在启动之前对车进行的设置，比如调节座椅的位置或设置你的后视镜，而参数就像是车辆在行驶过程中自动调整的东西，比如速度和刹车。

###### · 如何调整超参数？

可以通过多种方式进行：

1. **手动调整**：根据经验和直觉选择超参数的值，运行模型，查看效果，然后根据模型的性能来手动微调这些值。
2. **网格搜索（Grid Search）**：为每个超参数设定一个值的列表，然后尝试所有可能的组合，选择表现最好的一组。
3. **随机搜索（Random Search）**：在超参数的可能取值范围内随机选择组合，这种方法比网格搜索要快，尤其是当有很多超参数时。
4. **贝叶斯优化（Bayesian Optimization）**：这种方法使用概率模型来预测给定超参数组合的性能，并选择新的超参数以改进模型性能。
5. **自动化机器学习（AutoML）工具**：如Google的AutoML、Auto-sklearn等，它们可以自动为你的模型选择最佳超参数。

通常，调整超参数是一个迭代的过程，需要反复实验来找到最佳的参数组合。在这个过程中，交叉验证是常用的一种方法，它可以帮助评估不同超参数设置下模型的性能，以便选择最好的设置。



### Random Forest 随机森林

##### 1. 什么是随机森林（Random Forest）？

一种基于决策树的集成学习算法，它通过组合多个决策树来提高预测的准确性和稳定性。随机森林算法的关键在于它如何引入多样性：

1. 每棵树都是从训练集的自助样本（bootstrap sample）中训练出来的。这意味着每棵树的训练数据都是通过从原始训练集中进行有放回的随机抽样得到的，大约63.2%的原始数据将在每个自助样本中出现。
2. 每棵树在决定划分时只考虑一个随机选择的属性子集，这进一步增加了模型之间的差异性。

随机森林是目前最受欢迎的机器学习算法之一，因为它通常能提供很好的性能，而且对超参数的设置不太敏感，易于使用。

##### 2. 应用：一段Python代码来演示如何实践这些概念

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

# Load the Iris dataset
X, y = load_iris(return_X_y=True)

# Initialize the Random Forest classifier with some parameters：1.随机森林中构建和训练决策树的数量 2.每棵树的最大深度
classifier = RandomForestClassifier(n_estimators=100, max_depth=3)

# Perform cross-validation
scores = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Fit the model on the training data
    classifier.fit(X_train, y_train)

    # Predict on the test data
    pred = classifier.predict(X_test)

    # Calculate the accuracy score
    score = metrics.f1_score(y_test, pred, average="weighted")
    scores.append(score)

# Print the average cross-validation score
print("Average cross-validation score {0:.2f}".format(np.average(scores)))

```

关于随机森林参数设置的解释：

在这段代码中，`RandomForestClassifier` 是初始化随机森林分类器的命令，其中包含两个超参数的设置：

1. **n_estimators=100**:
   - 这个参数指定了随机森林中决策树的数量。在这里设置为100，意味着将会有100棵决策树被构建和训练。这些树将独立地学习数据集，并最终他们的预测将会被集成（例如，通过投票）来决定最终的分类结果。通常来说，更多的树可能会提升模型的性能和稳定性，但同时也会增加计算成本。
2. **max_depth=3**:
   - 这个参数指定了每棵树的最大深度。深度是指从根到叶子的最长路径上的边数。最大深度为3意味着每棵树将被限制为最多三层。这有助于防止过拟合，因为它限制了模型复杂度；但如果设置得太低，也可能会导致欠拟合，即模型可能不能捕捉数据中的所有模式。通过设置最大深度，我们可以控制决策树的生长，使其在足够学习数据的同时不会变得过于复杂。

其余部分和决策树几乎具有相同的逻辑和操作（跳转看上面）

##### 3. 引入K-NN算法：Out of Bag (OOB) 错误和基于实例的学习

###### · OOB（Out of Bag) ：现象/error

- 在随机森林算法中，每棵树都是通过有放回抽样（bootstrap sampling）从训练集中生成的，这意味着某些样本可能被多次选中，而另一些则可能一次也没被选中。那些没有被选中用于训练某棵树的样本被称为OOB样本。因为这些样本没有参与到树的训练中，它们可以被用作验证集来评估模型的性能，而无需显式地从训练集中分离出一部分数据来作为验证集。
- OOB错误是指在这些OOB样本上评估模型时计算出的错误率，它可以作为模型泛化能力的一个估计。 



### K-NN 最近邻分类器（“基于实例的学习”算法模型）

##### 1. 基于实例的学习

- 这类学习方法不会创建一个普遍适用的模型，而是直接使用训练集来进行预测。
- 在k-NN算法中，当有一个未标记的新实例需要分类时，算法会在训练集中寻找最相似的k个实例，然后根据这些最近邻的类别来预测新实例的类别，通常是采用多数投票法。

##### 2. 距离函数

- 距离函数在k-NN算法中至关重要，因为它们定义了数据点之间的相似性。
- 几种常见的距离函数包括：
  - **闵可夫斯基距离（Minkowski）**：是一个广义的距离度量，包含了其他距离度量作为其特殊情况。
  - **欧几里得距离（Euclidean）**：最常见的距离度量，计算两点间的直线距离。
  - **曼哈顿距离（Manhattan）**：计算在坐标系中的水平或垂直距离，类似于在城市中驾车时需要遵循街道格局绕行的距离。
  - 以及其他距离度量，如切比雪夫距离（Chebyshev）、堪培拉距离（Canberra）等。

在一些情况下，可能会给距离函数中的属性加权，以反映不同属性的重要性。这意味着在计算相似性时，某些特征可能会被视为比其他特征更重要。

##### 3. 应用：一段Python代码来演示如何实践这些概念

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

# 载入鸢尾花数据集
X, y = load_iris(return_X_y=True)

# 初始化k-NN分类器，设置邻居数为3
classifier = KNeighborsClassifier(n_neighbors=3)

# 准备存储每次交叉验证的分数
scores = []

# 设置KFold为5次交叉验证，并打乱数据
kf = KFold(n_splits=5, shuffle=True)

# 进行交叉验证
for train_index, test_index in kf.split(X):
    # 分割数据为训练集和测试集
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # 在训练集上训练模型
    classifier.fit(X_train, y_train)

    # 在测试集上进行预测
    pred = classifier.predict(X_test)

    # 计算并存储F1分数，这里使用加权平均来处理多分类问题
    score = metrics.f1_score(y_test, pred, average="weighted")
    scores.append(score)

# 计算所有交叉验证迭代的平均F1分数，并打印
print("Average cross-validation score {0:.2f}".format(np.average(scores)))
```



## 知识表示（Knowledge Representation）<Model, Representation>

- “知识表示”是什么，即机器学习模型如何理解和处理数据中的模式或概念。
- 介绍了各种类型的知识表示方法，从基础的（如规则集、决策树、最近邻、线性分类）到高级的方法（如Kernel methods、神经网络）

<img src="https://cdn.jsdelivr.net/gh/boyan-uni/pic-bed/img/MachineLearning-L02-P3.png" alt="image-20240812155941942" style="width:70%;" />


上图展示了规则集、决策树和1-最近邻分类器在机器学习中的表示方法和应用。

1. **规则集和决策树**:
   - 展示了一个决策树模型的结构，包括决策节点和它们的条件（例如：`X<0.5`）。
   - 可视化解释了决策树是如何将特征空间划分成不同区域的，每个区域代表不同的类别（星星、三角形）。
   - 演示了决策树的规则集表示，其中包括一组规则和一个默认规则。规则是用来预测新数据点的类别，根据数据点的特征与规则的匹配情况。
2. **1-最近邻分类器/逻辑回归**:
   - 1-最近邻分类器的模型部分给出了四个点的坐标及其类别，它的工作原理是找到与新数据点最接近的一个训练点，并预测新数据点的类别与最近点相同。
   - 下方的图表展示了如何基于1-最近邻的原理划分特征空间，不同颜色的区域代表不同的类别。
   - 另一部分图表显示了逻辑回归的结果，这是一种线性模型，用直线划分不同的类别。

这张图片总结了这些模型如何在二维空间中表示知识，并用来做出分类决策。它说明了每种模型的决策边界是如何形成的，以及这些边界如何影响新数据点的分类。







































